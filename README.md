# maritime-chatter-generation
The maritime industry is the backbone of global trade with more than 80% of the trade volume transported by sea. The safety and the efficiency of the maritime trade suffer from maritime disasters, mainly due to human errors. Therefore, accurate and
effective communication in the maritime industry is critical for safety and operational efficiency. Automatic Speech Recognition systems can facilitate better maritime communication. However, an open-source Automatic Speech Recognition training data
set in this domain is not available and creating such a dataset containing the chatter and its audio requires significant amount of manual effort. Creating this dataset can be achieved by using a Large Language Model to generate maritime chatters
and a Text-to-Speech model to create the audio data. To address the problem of data shortage, Llama 3.1 8B model is used to augment an existing manually created dataset of radio chatter instances with an adaptation of the
[Self-Instruct method](https://doi.org/10.48550/arXiv.2212.10560) developed by Wang et. al (2023). Afterwards, the model is fine-tuned with the augmented dataset using Low-Rank Adaptation and prompt-tuning. Results prove that Large Language Models
can be fine-tuned with Low-Rank Adaptation and prompt-tuning to generate realistic and unique maritime radio chatters that comply with the regulations defined in 
[IMO Standard Maritime Communication Phrases](https://wwwcdn.imo.org/localresources/en/OurWork/Safety/Documents/A.918(22).pdf). Additionally, four evaluation metrics are proposed to assess the correctness of the format, information accuracy, 
logical coherence and the uniqueness of the maritime distress calls.

See Methodology.md for the details of the methodology.

# Folders

all_countries: allCountries.txt file from the geonames data must be in this folder.

data: Contains the Keywords for the Keyword filter and vessel dataset.

eval_logic: Contains the evaluations of logical coherence metric.

	eval_logic/{task_name}: Evaluation of chatters generated by LoRA adapters 
	eval_logic/prompt_tuning/{task_name}: Evaluation of chatters generated by prompt tuning adapters
	eval_logic/vanilla: Evaluation of Chatters generated by the vanilla LLM

evaluation: Contains the evaluations of other metrics.

	evaluation/{task_name}: Evaluation of chatters generated by LoRA adapters 
	evaluation/prompt_tuning/{task_name}: Evaluation of chatters generated by prompt tuning adapters
	evaluation/vanilla: Evaluation of Chatters generated by the vanilla LLM

experiments: Contains the snythetic training Chatters.

GSHHS_dataset: Contains the GSHHS dataset.

models: Contains the trained LoRA and prompt tuning adapters.

	models/{task_name}: LoRA adapters 
	models/prompt_tuning/{task_name}: Prompt tuning Adapters

prompts: Contains the instruction part of the instance generation prompts used in the Self-Instruct pipeline.

scripts: Contains the code.

seed_outputs: Contains the seed instances. seed_outputs.json contains the instances. seed_input.json contains the inputs only. seed_inspect.json contains the context but with more details for possible manual inspections of the contexts.

synthetic_chatters: Contains the 100 synthetic Chatters generated by LoRA Adapters, prompt tuning adapters and vanilla model.

	synthetic_chatters/{task_name}: Synthetic chatters generated by LoRA adapters 
	synthetic_chatters/prompt_tuning/{task_name}: Synthetic chatters generated by prompt tuning adapters
	synthetic_chatters/vanilla: Synthetic chatters generated by the vanilla LLM	

# Scripts

### geo_test.py ###

Context Generation from the Pipeline mentioned above

### generate_instances_unsloth.py ###

This is the self-instruct pipeline code explained in the methodology chapter. It includes instance generation and filtering. Saves the synthetic chatter to the path experiments/{task_name}.

### lora_finetune.ipynb ###

LoRA code. Saves the Adapters to the folder models{task_name}.

### prompt_tuning.ipynb ###

Prompt tuning code. Saves the Adapters to the folder models/prompt_tuning/{task_name}.

### run_models.py, run_models_prompt_tuning.py, run_models_vanilla.py ###

Loads the LoRA Adapters, prompt tuning adapters and vanilla model respectively. 100 synthetic distress calls are generated and saved to paths synthetic_chatters/{task_name}, synthetic_chatters/prompt_tuning/{task_name} and synthetic_chatters/vanilla respectively.

### inspect_model.py, inspect_model_prompt_tuning.py, inspect_model_vanilla.py ###

Loads the synthetic calls generated by LoRA adapters, prompt tuning adapters and vanilla model respectively. 100 synthetic distress calls are evaluated and the results ar saved to paths evaluation/{task_name}, evaluation/prompt_tuning/{task_name} and evaluation/vanilla respectively.

###  prepare_evaluation_form.py ###

Prepares the Logical coherence evaluation formular with 10 randomly sampled synthetic chatters from the specified synthetic chatter path.




---
Written by: GÃ¼rsel Akdeniz, Emin Nakilcioglu
Last updated: 10.09.2025
---
# Maritime Chatter Generation

## ðŸ“‹ Project Overview

The maritime industry handles over 80% of global trade by volume, but safety is compromised by human errors. This project addresses the lack of open-source Automatic Speech Recognition (ASR) datasets for maritime radio communication by using Large Language Models (LLMs) to generate synthetic maritime distress calls, which can be converted to audio using Text-to-Speech systems.

We use **Llama 3.1 8B** with an adapted [Self-Instruct method](https://doi.org/10.48550/arXiv.2212.10560) to augment manually created seed instances. The model is fine-tuned using:
- **Low-Rank Adaptation (LoRA)**
- **Prompt Tuning**

Evaluation metrics include format correctness, information accuracy, and uniqueness.

ðŸ“˜ See `Methodology.md` for detailed technical information.

---

## ðŸš€ Getting Started

### Prerequisites

1. **Download Required Datasets:**
   - [GSHHG Dataset](https://www.soest.hawaii.edu/pwessel/gshhg/)
     - Place `GSHHS_shp/f/GSHHS_f_L1.shp` in `GSHHS_dataset/`
   - [GeoNames allCountries.zip](https://download.geonames.org/export/dump/)
     - Extract and place `allCountries.txt` in `all_countries/`

2. **Install Dependencies:**
   ```bash
   pip install -r requirements.txt
   ```

---

## ðŸ“ Folder Structure

| Folder | Description |
|--------|-------------|
| `all_countries/` | GeoNames dataset (`allCountries.txt`) |
| `data/` | Vessel dataset and keyword filters |
| `evaluation/` | Evaluation results for different models |
| `experiments/` | Synthetic training data from Self-Instruct pipeline |
| `GSHHS_dataset/` | GSHHS coastline shapefiles |
| `models/` | Trained adapters (LoRA and prompt tuning) |
| `prompts/` | Instruction templates for Self-Instruct |
| `scripts/` | All Python scripts and Jupyter notebooks |
| `seed_outputs/` | Initial seed instances per SMCP category |
| `synthetic_chatters/` | Generated 100-sample chatter sets |

### Subfolder Organization

| Path | Content |
|------|---------|
| `evaluation/{task_name}` | LoRA adapter evaluations |
| `evaluation/prompt_tuning/{task_name}` | Prompt tuning evaluations |
| `models/{task_name}` | LoRA trained adapters |
| `models/prompt_tuning/{task_name}` | Prompt tuning adapters |
| `synthetic_chatters/{task_name}` | Chatters from LoRA model |
| `synthetic_chatters/prompt_tuning/{task_name}` | Chatters from prompt-tuned model |

---

## âš™ï¸ Script Overview & Usage

> ðŸ’¡ **Before running:** Edit file paths and parameters at the top of each script!

### 1. Context Generation
```python
# geo_test.py
```
- **Purpose:** Generate geographical contexts and scenarios
- **Configuration:** Shapefile paths, GeoNames data, vessel data paths
- **Usage:** Called internally by other scripts

### 2. Data Generation
```python
# generate_instances.py
```
- **Purpose:** Run Self-Instruct pipeline to create synthetic training data
- **Configuration:** 
  - `land_shapefile`, `geonames_data_path`, `vessel_data_path`
  - `task_name`, `prompt_file`, LLM path, seed output path
  - Number of instances to generate
- **Order:** Run first (after preparing data/seed instances)
- **Output:** `experiments/{task_name}/`

### 3. LoRA Fine-Tuning
```python
# lora_finetune.ipynb
```
- **Purpose:** Fine-tune Llama 3.1 8B using LoRA
- **Configuration:** 
  - `model_dir` (base LLM)
  - `chatter_path` (from step 2)
  - `task_name`, hyperparameters
- **Order:** After `generate_instances.py`
- **Output:** `models/{task_name}/`

### 4. Prompt Tuning
```python
# prompt_tuning.ipynb
```
- **Purpose:** Fine-tune using prompt tuning technique
- **Configuration:** 
  - `model_dir`, `chatter_path`, `task_name`
  - `prompt_file`, hyperparameters
- **Order:** Alternative to LoRA, after `generate_instances.py`
- **Output:** `models/prompt_tuning/{task_name}/`

### 5. Generate Synthetic Chatters (LoRA)
```python
# run_models.py
```
- **Purpose:** Generate 100 new chatters using LoRA adapters
- **Configuration:** 
  - `hyperparameters_path` (from LoRA output)
  - File paths for input/output
- **Order:** After LoRA fine-tuning
- **Output:** `synthetic_chatters/{task_name}/`

### 6. Generate Synthetic Chatters (Prompt Tuning)
```python
# run_models_prompt_tuning.py
```
- **Purpose:** Generate 100 new chatters using prompt tuning adapters
- **Configuration:** 
  - `hyperparameters_path` (from prompt tuning output)
  - `prompt_file`, file paths
- **Order:** After prompt tuning
- **Output:** `synthetic_chatters/prompt_tuning/{task_name}/`

### 7. Evaluate LoRA Results
```python
# inspect_model.py
```
- **Purpose:** Evaluate chatters generated by LoRA adapters
- **Configuration:** 
  - `keywords_path`, `ship_data_path`, `land_shapefile`
  - `geonames_data_path`, `seed_path`, `synthetic_chatter_path`
  - `save_dir`
- **Order:** After generating LoRA chatters
- **Output:** `evaluation/{task_name}/`

### 8. Evaluate Prompt Tuning Results
```python
# inspect_model_prompt_tuning.py
```
- **Purpose:** Evaluate chatters from prompt-tuned adapters
- **Configuration:** Same as above, using prompt-tuned paths
- **Order:** After generating prompt tuning chatters
- **Output:** `evaluation/prompt_tuning/{task_name}/`

---

## ðŸ”„ Execution Workflow

1. **Prepare Data**
   - Download and place GSHHG and GeoNames datasets
   - Prepare seed chatters in `seed_outputs/`

2. **Generate Training Data**
   ```bash
   python generate_instances.py
   ```

3. **Model Fine-Tuning**
   - **Option A (LoRA):** Run `lora_finetune.ipynb`
   - **Option B (Prompt Tuning):** Run `prompt_tuning.ipynb`

4. **Generate Synthetic Chatters**
   - **For LoRA:** `python run_models.py`
   - **For Prompt Tuning:** `python run_models_prompt_tuning.py`

5. **Evaluate Results**
   - **LoRA Evaluation:** `python inspect_model.py`
   - **Prompt Tuning Evaluation:** `python inspect_model_prompt_tuning.py`

---

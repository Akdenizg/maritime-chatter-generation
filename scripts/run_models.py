import geo_test
from unsloth import FastLanguageModel
import datetime
import json
import os

current_time = datetime.datetime.now().strftime("%Y%m%d_%H%M")

land_shapefile = "./GSHHS_dataset/GSHHS_shp/f/GSHHS_f_L1.shp" # path of the GSHHS data
geonames_data_path = './all_countries/allCountries.txt' # path of the geonames data
ship_data_path = './data/ship_data_dk_us.pkl' # path of the ship data

hyperparameters_path = "./models/.../hyperparameters.json" # path of the hyperparameters.json file generated by lora_finetune notebook.

save_dir = "./" # project folder. A folder with name "synthetic_chatters" will be opened and the results will be saved in it.


with open(hyperparameters_path, 'r') as file:
    hyperparameters = json.load(file)

model_name = hyperparameters["model_path"]
training_chatter_path = hyperparameters["chatter_path"]
use_lora_adapter = hyperparameters["is_lora"]
load_in_4bit = hyperparameters["load_in_4bit"]

# Load the JSON data
with open(training_chatter_path, 'r') as file:
    data = json.load(file)

if use_lora_adapter:
    #from unsloth import FastLanguageModel
    model, tokenizer = FastLanguageModel.from_pretrained(
        model_name = model_name,
        max_seq_length = 2048,
        dtype = None,
        load_in_4bit = load_in_4bit,
    )
    FastLanguageModel.for_inference(model) # Enable native 2x faster inference
prompt = """Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.

### Instruction:
{}

### Input:
{}

### Output:
{}"""

results = []
used_mmsi_list = [instance["input"]["vessel_MMSI"] for instance in data[data["task_name"]]["instances"]]

# Run inference 500 times
for i in range(100):
    scenario_inspect, scenario_input = geo_test.MaritimeAnalysis(land_shapefile, geonames_data_path, ship_data_path, data["task_name"]).execute(used_mmsi_list)

    inputs = tokenizer(
        [
            prompt.format(
                data[data['task_name']]['instruction'], # instruction
                str(scenario_input), # input
                "", # output - leave this blank for generation!
            )
        ], return_tensors = "pt").to("cuda")

    outputs = model.generate(
                **inputs,
                max_new_tokens=400,
                use_cache=True,
                # do_sample=True,
                # top_p=0.9,
                # temperature=0.9,
                # min_length=None,
                # top_k=400,
                # repetition_penalty=1,
                # length_penalty=1,
            )
    generated_output = tokenizer.batch_decode(outputs)[0]
    results.append({
        "id": i + 1,
        "inspection": scenario_inspect,
        "input": scenario_input,
        "output": generated_output.replace("<|end_of_text|>","").split("Output:")[1].strip().split("\n")
    })

    i += 1
    print(i)

data_to_save = {
        "task_name": hyperparameters["task_name"],
        "hyperparameters": hyperparameters,
        "results": results
}
if not os.path.exists(f'{save_dir}/synthetic_chatters/{hyperparameters["task_name"]}'):
    os.mkdir(f'{save_dir}/synthetic_chatters/{hyperparameters["task_name"]}')
# Save results to JSON
with open(f'{save_dir}/synthetic_chatters/{hyperparameters["task_name"]}/{current_time}.json', 'w') as outfile:
    json.dump(data_to_save, outfile, ensure_ascii=False, indent=4)
{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "#!pip install unsloth \"xformers==0.0.28.post2\"\n",
    "# Also get the latest nightly Unsloth!\n",
    "#!pip uninstall unsloth -y && pip install --upgrade --no-cache-dir \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
    "#!pip install unsloth==2025.3.6 unsloth_zoo==2025.3.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting the model and the tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mguersel-akdeniz\u001b[0m (\u001b[33mcml-marfm\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.8"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/gakdeniz/Dev/ma-llm-tuning/scripts/wandb/run-20250327_074026-c8hc9gk0</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/cml-marfm/llama_finetuning/runs/c8hc9gk0' target=\"_blank\">radiant-water-168</a></strong> to <a href='https://wandb.ai/cml-marfm/llama_finetuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/cml-marfm/llama_finetuning' target=\"_blank\">https://wandb.ai/cml-marfm/llama_finetuning</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/cml-marfm/llama_finetuning/runs/c8hc9gk0' target=\"_blank\">https://wandb.ai/cml-marfm/llama_finetuning/runs/c8hc9gk0</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.3.6: Fast Llama patching. Transformers: 4.49.0.\n",
      "   \\\\   /|    NVIDIA RTX A6000. Num GPUs = 1. Max memory: 47.319 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 8.6. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post3. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19a87a9c4d33410ab614a1a114f2ad14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "import json\n",
    "import datetime\n",
    "import os\n",
    "\n",
    "max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "r = 256 # Rank of the LoRA adapters.\n",
    "\n",
    "task_name = \"task_name\" # Choose from [\"reporting_fire\", \"reporting_flooding\", \"reporting_collision\", \"reporting_grounding\":\n",
    "                            #              \"reporting_list-danger_of_capsizing\", \"reporting_sinking\", \"reporting_attack\",\n",
    "                            #              \"reporting_person_overboard\", \"reporting_drift\", \"reporting_undesignated_distress\"]\n",
    "\n",
    "load_in_4bit = False # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "chatter_path = './experiments/./...._new_outputs.json' # Synthetic finetuning data to be used as finetuning dataset. Generated by generate_instances.py\n",
    "\n",
    "### Training hyperparameters ###\n",
    "num_epoch = 10\n",
    "learning_rate = 2e-4\n",
    "lr_scheduler_type = \"linear\"\n",
    "warmup_steps = 30\n",
    "use_lora_adapter = True\n",
    "is_wandb = True\n",
    "\n",
    "### Model variables ###\n",
    "model_dir = \"path-to-base-llm\"\n",
    "used_model = \"Llama-3.1-8B\" # model_name_for_documentation, change if you use a different LLM\n",
    "\n",
    "### BELOW VARIABLES CAN STAY THE SAME ###\n",
    "model_path = f\"./models/{task_name}\"\n",
    "results_path = f\"{model_path}/{used_model}\"#f'{model_path}/{used_model}'\n",
    "\n",
    "\n",
    "config = {\n",
    "        \"max_seq_length\": max_seq_length,\n",
    "        \"dtype\": dtype,\n",
    "        \"load_in_4bit\": load_in_4bit,\n",
    "        \"model_dir\": model_dir,\n",
    "        \"used_model\": used_model,\n",
    "        \"model_path\": model_path,\n",
    "        \"r\": r,\n",
    "        \"chatter_path\": chatter_path,\n",
    "        \"results_path\": results_path,\n",
    "        \"num_epoch\": num_epoch,\n",
    "        \"learning_rate\": learning_rate,\n",
    "        \"lr_scheduler_type\": lr_scheduler_type,\n",
    "        \"warmup_steps\": warmup_steps,\n",
    "        \"use_lora_adapter\": use_lora_adapter\n",
    "    }\n",
    "\n",
    "if is_wandb:\n",
    "    import wandb\n",
    "    wandb.init(project=\"llama_finetuning\", reinit=True, config= config)\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = model_dir,\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting the lora adapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2025.3.6 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "if use_lora_adapter:\n",
    "    model = FastLanguageModel.get_peft_model(\n",
    "        model,\n",
    "        r = r, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "        target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                        \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "        lora_alpha = 16,\n",
    "        lora_dropout = 0, # Supports any, but = 0 is optimized\n",
    "        bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
    "        # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
    "        use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
    "        random_state = 3407,\n",
    "        use_rslora = False,  # We support rank stabilized LoRA\n",
    "        loftq_config = None, # And LoftQ\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparing the dataset and the prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "# Load your JSON data\n",
    "with open(chatter_path, 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{}\n",
    "\n",
    "Input:\n",
    "{}\n",
    "\n",
    "### Output:\n",
    "{}\"\"\"\n",
    "\n",
    "EOS_TOKEN = \"<|end_of_text|>\"\n",
    "\n",
    "def formatting_prompts(data):\n",
    "    instruction = data[task_name][\"instruction\"]\n",
    "    instances = data[task_name][\"instances\"]\n",
    "    formatted_data = []\n",
    "\n",
    "    for instance in instances:\n",
    "        input_data = instance[\"input\"]\n",
    "        output_data = instance[\"output\"]\n",
    "        input_text = json.dumps(input_data, ensure_ascii=False, indent=4)  # Convert input dictionary to a JSON-formatted string\n",
    "        output_text = '\\n'.join(output_data)  # Join output list into a single string\n",
    "\n",
    "        text = prompt.format(instruction, input_text, output_text) + EOS_TOKEN\n",
    "        formatted_data.append({\n",
    "            \"instruction\": instruction,\n",
    "            \"input\": input_text,\n",
    "            \"output\": output_text,\n",
    "            \"text\": text\n",
    "        })\n",
    "    \n",
    "    return formatted_data\n",
    "\n",
    "def formatting_prompts_all(files):\n",
    "    formatted_data = []\n",
    "    for chatter_file in files:\n",
    "        with open(chatter_file, 'r') as file:\n",
    "            data = json.load(file)\n",
    "        instruction = data[data[\"task_name\"]][\"instruction\"]\n",
    "        instances = data[data[\"task_name\"]][\"instances\"]\n",
    "\n",
    "        for instance in instances:\n",
    "            input_data = instance[\"input\"]\n",
    "            output_data = instance[\"output\"]\n",
    "            input_text = json.dumps(input_data, ensure_ascii=False, indent=4)  # Convert input dictionary to a JSON-formatted string\n",
    "            output_text = '\\n'.join(output_data)  # Join output list into a single string\n",
    "\n",
    "            text = prompt.format(instruction, input_text, output_text) + EOS_TOKEN\n",
    "            formatted_data.append({\n",
    "                \"instruction\": instruction,\n",
    "                \"input\": input_text,\n",
    "                \"output\": output_text,\n",
    "                \"text\": text\n",
    "            })\n",
    "    \n",
    "    return formatted_data\n",
    "\n",
    "# Create the formatted dataset\n",
    "formatted_dataset = formatting_prompts(data)\n",
    "\n",
    "# Convert the formatted data into a Hugging Face Dataset\n",
    "dataset = Dataset.from_list(formatted_dataset)\n",
    "\n",
    "#dataset = dataset.shuffle(seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'instruction': 'Generate a maritime radio chatter. A vessel makes a distress call and reports armed attack/piracy.',\n",
       " 'input': '{\\n    \"vessel_name\": \"HERUN ZHEJIANG\",\\n    \"vessel_MMSI\": \"four seven seven four three nine five zero zero\",\\n    \"vessel_call_sign\": null,\\n    \"vessel_type\": \"Motor Vessel\",\\n    \"vessel_coordinate_dms\": \"three two degrees five six point eight three minutes South, five one degrees two four minutes West\",\\n    \"compass_direction\": \"south east\",\\n    \"closest_place_name\": \"El Aduar\",\\n    \"distance_to_nearest_place\": \"five nine\",\\n    \"closest_place_country\": \"Brazil\",\\n    \"distance_to_nearest_port\": \"six zero\",\\n    \"nearest_port\": \"EstaÃ§Ã£o Naval do Rio Grande\",\\n    \"distance_to_nearest_harbor\": \"one six eight\",\\n    \"nearest_harbor\": \"Puerto de la Paloma\",\\n    \"digit_by_digit\": true,\\n    \"can_have_cargo\": null,\\n    \"closest_water_body\": null\\n}',\n",
       " 'output': 'Mayday, Mayday, Mayday. This is motor vessel HERUN ZHEJIANG, MMSI four seven seven four three nine five zero zero, at position three two degrees five six point eight three minutes South, five one degrees two four minutes West. We are under attack by pirates. I require immediate military assistance.\\nHERUN ZHEJIANG, this is Coast Guard. What is the current status of the vessel?\\nPirates are boarding our vessel. We have limited control of the ship and are drifting towards El Aduar, approximately five nine nautical miles away. We require immediate assistance.\\nUnderstood, HERUN ZHEJIANG. Military assistance is on its way. ETA is approximately four five minutes. Can you hold your current position?\\nNegative, Coast Guard. We are unable to maintain our position. Pirates have disabled our engines and are attempting to gain control of the bridge. Over.\\nRoger, we have dispatched a patrol boat and a helicopter to your location. Hold tight, assistance is on the way.',\n",
       " 'text': 'Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nGenerate a maritime radio chatter. A vessel makes a distress call and reports armed attack/piracy.\\n\\nInput:\\n{\\n    \"vessel_name\": \"HERUN ZHEJIANG\",\\n    \"vessel_MMSI\": \"four seven seven four three nine five zero zero\",\\n    \"vessel_call_sign\": null,\\n    \"vessel_type\": \"Motor Vessel\",\\n    \"vessel_coordinate_dms\": \"three two degrees five six point eight three minutes South, five one degrees two four minutes West\",\\n    \"compass_direction\": \"south east\",\\n    \"closest_place_name\": \"El Aduar\",\\n    \"distance_to_nearest_place\": \"five nine\",\\n    \"closest_place_country\": \"Brazil\",\\n    \"distance_to_nearest_port\": \"six zero\",\\n    \"nearest_port\": \"EstaÃ§Ã£o Naval do Rio Grande\",\\n    \"distance_to_nearest_harbor\": \"one six eight\",\\n    \"nearest_harbor\": \"Puerto de la Paloma\",\\n    \"digit_by_digit\": true,\\n    \"can_have_cargo\": null,\\n    \"closest_water_body\": null\\n}\\n\\n### Output:\\nMayday, Mayday, Mayday. This is motor vessel HERUN ZHEJIANG, MMSI four seven seven four three nine five zero zero, at position three two degrees five six point eight three minutes South, five one degrees two four minutes West. We are under attack by pirates. I require immediate military assistance.\\nHERUN ZHEJIANG, this is Coast Guard. What is the current status of the vessel?\\nPirates are boarding our vessel. We have limited control of the ship and are drifting towards El Aduar, approximately five nine nautical miles away. We require immediate assistance.\\nUnderstood, HERUN ZHEJIANG. Military assistance is on its way. ETA is approximately four five minutes. Can you hold your current position?\\nNegative, Coast Guard. We are unable to maintain our position. Pirates have disabled our engines and are attempting to gain control of the bridge. Over.\\nRoger, we have dispatched a patrol boat and a helicopter to your location. Hold tight, assistance is on the way.<|end_of_text|>'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[499]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting up the trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5bc2da9acd04054b27b399b62a69d33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing to [\"text\"] (num_proc=2):   0%|          | 0/500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "from unsloth import is_bfloat16_supported\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = dataset,\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dataset_num_proc = 2,\n",
    "    packing = False, # Can make training 5x faster for short sequences.\n",
    "    args = TrainingArguments(\n",
    "        per_device_train_batch_size = 2,\n",
    "        gradient_accumulation_steps = 4,\n",
    "        warmup_steps = warmup_steps,\n",
    "        num_train_epochs = num_epoch, # Set this for 1 full training run.\n",
    "        #max_steps = None,\n",
    "        learning_rate = learning_rate,\n",
    "        fp16 = not is_bfloat16_supported(),\n",
    "        bf16 = is_bfloat16_supported(),\n",
    "        logging_steps = 1,\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = lr_scheduler_type,\n",
    "        seed = 3407,\n",
    "        output_dir = \"outputs\",\n",
    "        report_to = \"wandb\", # Use this for WandB etc\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU = NVIDIA RTX A6000. Max memory = 47.319 GB.\n",
      "15.08 GB of memory reserved.\n"
     ]
    }
   ],
   "source": [
    "#@title Show current memory stats\n",
    "gpu_stats = torch.cuda.get_device_properties(0)\n",
    "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
    "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
    "print(f\"{start_gpu_memory} GB of memory reserved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 500 | Num Epochs = 15 | Total steps = 930\n",
      "O^O/ \\_/ \\    Batch size per device = 2 | Gradient accumulation steps = 4\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (2 x 4 x 1) = 8\n",
      " \"-____-\"     Trainable parameters = 20,971,520/8,051,232,768 (0.26% trained)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Will smartly offload gradients to save VRAM!\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m trainer_stats \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/chatter_generation/lib/python3.10/site-packages/transformers/trainer.py:2241\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2239\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   2240\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2241\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2242\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2243\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2244\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2245\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2246\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m<string>:306\u001b[0m, in \u001b[0;36m_fast_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n",
      "File \u001b[0;32m<string>:73\u001b[0m, in \u001b[0;36m_unsloth_training_step\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n",
      "File \u001b[0;32m~/miniforge3/envs/chatter_generation/lib/python3.10/site-packages/accelerate/accelerator.py:2329\u001b[0m, in \u001b[0;36mAccelerator.backward\u001b[0;34m(self, loss, **kwargs)\u001b[0m\n\u001b[1;32m   2327\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlomo_backward(loss, learning_rate)\n\u001b[1;32m   2328\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2329\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/chatter_generation/lib/python3.10/site-packages/torch/_tensor.py:626\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    616\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    617\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    618\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    619\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    624\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    625\u001b[0m     )\n\u001b[0;32m--> 626\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    627\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    628\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/chatter_generation/lib/python3.10/site-packages/torch/autograd/__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/chatter_generation/lib/python3.10/site-packages/torch/autograd/graph.py:823\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    821\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    822\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 823\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    824\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    825\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    826\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    827\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2563.3863 seconds used for training.\n",
      "42.72 minutes used for training.\n",
      "Peak reserved memory = 22.6 GB.\n",
      "Peak reserved memory for training = 5.094 GB.\n",
      "Peak reserved memory % of max memory = 47.761 %.\n",
      "Peak reserved memory for training % of max memory = 10.765 %.\n"
     ]
    }
   ],
   "source": [
    "#@title Show final memory and time stats\n",
    "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
    "used_percentage = round(used_memory         /max_memory*100, 3)\n",
    "lora_percentage = round(used_memory_for_lora/max_memory*100, 3)\n",
    "print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
    "print(f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\")\n",
    "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
    "print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
    "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
    "print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inference with the adapter only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the adapter and the tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_time = datetime.datetime.now().strftime(\"%Y%m%d_%H%M\")\n",
    "\n",
    "if load_in_4bit:\n",
    "    quantization = '_4bit'\n",
    "else:\n",
    "    quantization = ''\n",
    "\n",
    "if use_lora_adapter:\n",
    "    lora = '_lora'\n",
    "else:\n",
    "    lora = ''\n",
    "\n",
    "model.save_pretrained(f\"{results_path}{quantization}{lora}_{current_time}\") # Local saving\n",
    "tokenizer.save_pretrained(f\"{results_path}{quantization}{lora}_{current_time}\")\n",
    "\n",
    "data_to_save = {\n",
    "        \"task_name\": task_name,\n",
    "        \"load_in_4bit\": load_in_4bit,\n",
    "        \"is_lora\": use_lora_adapter,\n",
    "        \"model_name\": used_model,\n",
    "        \"model_path\": f\"{results_path}{quantization}{lora}_{current_time}\",\n",
    "        \"r\": r,\n",
    "        \"chatter_path\": chatter_path,\n",
    "        \"warmup_steps\": warmup_steps,\n",
    "        \"num_epoch\": num_epoch,\n",
    "        \"learning_rate\": learning_rate,\n",
    "        \"lr_scheduler_type\": lr_scheduler_type,\n",
    "        \"trainer_stats\": trainer_stats,\n",
    "}\n",
    "\n",
    "# Save results to JSON\n",
    "with open(f'{results_path}{quantization}{lora}_{current_time}/hyperparameters.json', 'w') as outfile:\n",
    "    json.dump(data_to_save, outfile, ensure_ascii=False, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chatter_generation",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

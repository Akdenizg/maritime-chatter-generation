{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "#!pip install unsloth \"xformers==0.0.28.post2\"\n",
    "# Also get the latest nightly Unsloth!\n",
    "#!pip uninstall unsloth -y && pip install --upgrade --no-cache-dir \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting the model and the tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#from unsloth import FastLanguageModel\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, default_data_collator, get_linear_schedule_with_warmup\n",
    "from peft import get_peft_model, PromptTuningInit, PromptTuningConfig, TaskType\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import Dataset\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import os\n",
    "import datetime\n",
    "\n",
    "### Training hyperparameters ### \n",
    "device = \"cuda\"\n",
    "max_length = 3000\n",
    "lr = 1e-2\n",
    "num_epoch = 50\n",
    "batch_size = 2\n",
    "is_wandb = False\n",
    "load_in_4bit = True\n",
    "train_prompt = \"long\"\n",
    "save_epochs = 50\n",
    "num_virtual_tokens = 200 ### Specify here the number of virtual tokens to be trained\n",
    "max_length = num_virtual_tokens + max_length\n",
    "\n",
    "### Paths to define ###\n",
    "model_dir = \"path_to_base_model\"\n",
    "used_model = \"unsloth-Llama-3.1-8B\" # Model name for documentation. Change if you use a different LLM.\n",
    "model_path = \"./models\"\n",
    "chatter_path = './experiments/...new_outputs.json' # Synthetic chatter to be used as prompt tuning data\n",
    "prompt_file = \"./prompts/prompt_file.txt\" # Change to the path of the prompt file for the task category of the data you are finetuning with\n",
    "\n",
    "with open(chatter_path, 'r') as file:\n",
    "        chatters = json.load(file)\n",
    "task_name = chatters[\"task_name\"]\n",
    "results_path = f'./models/prompt_tuning/{task_name}/{used_model}'\n",
    "\n",
    "with open(chatter_path, 'r') as file:\n",
    "        chatters = json.load(file)\n",
    "peft_config = PromptTuningConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    prompt_tuning_init=PromptTuningInit.TEXT,\n",
    "    prompt_tuning_init_text= chatters[chatters[\"task_name\"]][\"instruction\"],\n",
    "    num_virtual_tokens=num_virtual_tokens,\n",
    "    tokenizer_name_or_path=model_dir,\n",
    ")\n",
    "\n",
    "current_time = datetime.datetime.now().strftime(\"%Y%m%d_%H%M\")\n",
    "\n",
    "if load_in_4bit:\n",
    "    quantization = '_4bit'\n",
    "else:\n",
    "    quantization = ''\n",
    "\n",
    "save_dir = f\"{results_path}{quantization}_{current_time}\"\n",
    "\n",
    "\n",
    "config = {\n",
    "        \"num_virtual_tokens\": num_virtual_tokens,\n",
    "        \"load_in_4bit\": load_in_4bit,\n",
    "        \"model_dir\": model_dir,\n",
    "        \"used_model\": used_model,\n",
    "        \"model_path\": model_path,\n",
    "        \"chatter_path\": chatter_path,\n",
    "        \"results_path\": results_path,\n",
    "        \"num_epoch\": num_epoch,\n",
    "        \"learning_rate\": lr\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`low_cpu_mem_usage` was None, now default to True since model is quantized.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "793f964122f946a3b97a727407ec40bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 819,200 || all params: 8,031,080,448 || trainable%: 0.0102\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=load_in_4bit,  # 4-bit quantization\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,  # bfloat16 for Ampere+, float16 for older GPUs\n",
    "    bnb_4bit_use_double_quant=False,  # Double quantization saves memory\n",
    ")\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
    "if tokenizer.pad_token_id is None:\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "# Load model and prepare for prompt tuning\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_dir,\n",
    "    quantization_config=quantization_config if load_in_4bit else None\n",
    ")\n",
    "if is_wandb:\n",
    "    import wandb\n",
    "    wandb.init(project=\"llama_finetuning\", reinit=True, config=config)\n",
    "\n",
    "model = get_peft_model(model, peft_config)\n",
    "print(model.print_trainable_parameters())  # Verify trainable parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparing the dataset and the prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset processing\n",
    "def preprocess_function(examples):\n",
    "    inputs = [f\"Instruction: {instruction}\\nInput: {input_text}\\nResponse:\"\n",
    "              for instruction, input_text in zip(examples[\"instruction\"], examples[\"input\"])]\n",
    "    targets = examples[\"output\"]\n",
    "\n",
    "    # Concatenate inputs and targets\n",
    "    full_texts = [text for text in examples[\"text\"]]\n",
    "\n",
    "    # Tokenize the concatenated inputs and targets with padding and truncation\n",
    "    tokenized_outputs = tokenizer(\n",
    "        full_texts,\n",
    "        max_length=max_length,\n",
    "        truncation=True,\n",
    "        padding='max_length',\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "\n",
    "    # Create labels by copying tokenized outputs\n",
    "    labels = tokenized_outputs['input_ids'].clone()\n",
    "\n",
    "    # Determine the length of the inputs to mask them in the labels\n",
    "    inputs_tokenized = tokenizer(\n",
    "        inputs,\n",
    "        max_length=max_length,\n",
    "        truncation=True,\n",
    "        padding='max_length',\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "\n",
    "    #Mask the input tokens in the labels\n",
    "    prompt_lengths = (inputs_tokenized['attention_mask'] > 0).sum(dim=1)\n",
    "    for i, prompt_length in enumerate(prompt_lengths):\n",
    "        labels[i, :prompt_length] = -100  # Mask input tokens\n",
    "\n",
    "    tokenized_outputs['labels'] = labels\n",
    "\n",
    "    return tokenized_outputs\n",
    "\n",
    "# Formatting your dataset\n",
    "def format_dataset(chatters):\n",
    "    with open(prompt_file, \"r\") as f:\n",
    "        base_prompt = \"\\n\".join(f.readlines())    \n",
    "\n",
    "    prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "    ### Instruction:\n",
    "    {}\n",
    "\n",
    "    ### Context:\n",
    "    {}\n",
    "\n",
    "    ### Maritime Radio Chatter:\n",
    "    {}\"\"\"\n",
    "\n",
    "    EOS_TOKEN = \"<|end_of_text|>\"\n",
    "\n",
    "    formatted_data = []\n",
    "    for instance in chatters[task_name][\"instances\"]:\n",
    "        instruction = chatters[chatters[\"task_name\"]][\"instruction\"] if train_prompt == \"short\" else base_prompt.replace(\"Task: \", \"\")\n",
    "        input_data = json.dumps(instance[\"input\"], ensure_ascii=False, indent=4)\n",
    "        output_data = '\\n'.join(instance[\"output\"])\n",
    "\n",
    "        formatted_data.append({\n",
    "            \"instruction\": instruction,\n",
    "            \"input\": input_data,\n",
    "            \"output\": output_data,\n",
    "            \"text\": prompt.format(instruction, input_data, output_data) + EOS_TOKEN,\n",
    "        })\n",
    "    \n",
    "    return Dataset.from_list(formatted_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1aab81940049433ea8ac5a991d0ad1e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load and preprocess dataset\n",
    "raw_dataset= format_dataset(chatters)\n",
    "processed_dataset = raw_dataset.map(preprocess_function, batched=True, remove_columns=[\"instruction\", \"input\", \"output\", \"text\"], num_proc=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'instruction': 'You are a creative expert of maritime industry and undesignated distresses of vessels. Generate a maritime radio chatter which complies with the IMO Standard marine Communication Phrases. A vessel makes a distress call and reports an undesignated distress.\\n\\n\\n\\nTips about creating original scenarios:\\n\\n- Describe different situations of the crew. Be creative.\\n\\n- Mention specific strategies the crew is using to cope with the undesignated distress. Be creative.\\n\\n- Provide ETAs which match with the vessel coordinate and the destination. Provide various types of help available from the Coast Guard. Be creative.\\n\\n- Make the conversation flow naturally. Do not make people say same phrases repeatedly.\\n\\n- Do not generate any other text but the radio chatter. Keep the generation limited to the radio chatter.\\n\\n- All distress calls must start with \"Mayday, Mayday, Mayday\". Ship gives its location in terms of degrees and tells the nature of disaster and the help needed.\\n\\n- Do not generate any other phrases in brackets or paranthesis.\\n\\n- Do not generate repetitive statements. For example, do not make the same party speak consecutively.\\n\\n- The vessel and the coast guard should speak in turns. Do not make the vessel or the coast guard speak multiple times consecutively. Turns of each party should be separated by \"\\\\n\".\\n\\n- Coast Guard should answer the distress call with one of the following phrases: \"This is Coast Guard\", \"Coast Guard here\", \"Coast Guard responding\".\\n\\n- Distress should not be about fire, explosion, flooding, grounding, sinking, armed attack, person overboard, listing or danger of capsizing.\\n\\nBelow are some example outputs and contexts. Contexts are dictionaries with keys \"vessel_name\", \"vessel_MMSI\", \"vessel_call_sign\", \"vessel_coordinate_dms\", \"digit_by_digit\",\\n\\n\"compass_direction\", \"closest_place_name\", \"distance_to_nearest_place\", \"closest_place_country\", \"distance_to_nearest_port\", \"nearest_port\", \"distance_to_nearest_harbor\",\\n\\n\"nearest_harbor\", \"closest_water_body\" and \"can_have_cargo\".\\n\\n\\n\\nTHIS PART IS VERY IMPORTANT:\\n\\n- You must use \"vessel_name\" and \"vessel_coordinate_dms\".\\n\\n- You must use \"vessel_MMSI\", \"vessel_call_sign\" and \"vessel_type\", as long as they are not null. Omit the null ones.\\n\\n- \"vessel_type\" should be indicated in the format \"[vessel_type] [vessel_name]\". For example, \"motor vessel [vessel_name]\". Do not use phrases \"I am a [vessel_type]\" or \"We are a [vessel_type]\" to indicate the vessel_type.\\n\\n- If \"vessel_MMSI\" is null, the vessel should not indicate its\\' MMSI number. If it is not null, the vessel\\'s MMSI should be indicated in the chatter.\\n\\n- If \"vessel_call_sign\" is null, the vessel should not indicate its\\' call sign or any other call sign in the chatter. If it is not null, the vessel\\'s call sign should be indicated in the chatter.\\n\\n- If \"can_have_cargo\" is null, do not mention any cargo or cargo hold in the chatter. If it is \\'True\\', then you can optionally mention cargo and/or cargo hold. Make the cargo suitable to the vessel type in that case.\\n\\n- If \"digit_by_digit\" is true, all numbers in the chatter should be expressed in digits, for example 454 as \"four five four\". If digit_by_digit is False, use full numbers.\\n\\n- Usage of the rest of the keys are optional.\\n\\n- Do not use both of the information about harbors and ports if they are both present. Use one of them only.\\n\\n- Do not use port and harbor when the vessel gives information about the its position at the beginning of the chatter, instead use them later in chatter optionally.', 'input': '{\\n    \"vessel_name\": \"CG21126\",\\n    \"vessel_MMSI\": null,\\n    \"vessel_call_sign\": null,\\n    \"vessel_type\": \"Law Enforcement Vessel\",\\n    \"vessel_coordinate_dms\": \"forty degrees three four decimal zero nine minutes North, thirty degrees four two decimal four seven minutes West\",\\n    \"compass_direction\": \"north east\",\\n    \"closest_place_name\": \"Ilha do Corvo\",\\n    \"distance_to_nearest_place\": \"fifty-four\",\\n    \"closest_place_country\": \"Portugal\",\\n    \"distance_to_nearest_port\": \"fifty-seven\",\\n    \"nearest_port\": \"Porto da Casa\",\\n    \"distance_to_nearest_harbor\": \"sixty-nine\",\\n    \"nearest_harbor\": \"Porto de Sao Pedro\",\\n    \"digit_by_digit\": false,\\n    \"can_have_cargo\": null,\\n    \"closest_water_body\": null\\n}', 'output': 'Mayday, Mayday, Mayday. This is law enforcement vessel CG21126. We are at position forty degrees three four decimal zero nine minutes North, thirty degrees four two decimal four seven minutes West. Requesting immediate assistance.\\nThis is Coast Guard. Please elaborate on the nature of your emergency.\\nOur vessel has suffered significant damage from a collision with a large container vessel. We are adrift and need immediate assistance to prevent further casualties.\\nUnderstood. Coast Guard is dispatching assistance from Porto da Casa. ETA is approximately 57 minutes. Maintain current position and remain on this channel for further instructions.', 'text': 'Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n    ### Instruction:\\n    You are a creative expert of maritime industry and undesignated distresses of vessels. Generate a maritime radio chatter which complies with the IMO Standard marine Communication Phrases. A vessel makes a distress call and reports an undesignated distress.\\n\\n\\n\\nTips about creating original scenarios:\\n\\n- Describe different situations of the crew. Be creative.\\n\\n- Mention specific strategies the crew is using to cope with the undesignated distress. Be creative.\\n\\n- Provide ETAs which match with the vessel coordinate and the destination. Provide various types of help available from the Coast Guard. Be creative.\\n\\n- Make the conversation flow naturally. Do not make people say same phrases repeatedly.\\n\\n- Do not generate any other text but the radio chatter. Keep the generation limited to the radio chatter.\\n\\n- All distress calls must start with \"Mayday, Mayday, Mayday\". Ship gives its location in terms of degrees and tells the nature of disaster and the help needed.\\n\\n- Do not generate any other phrases in brackets or paranthesis.\\n\\n- Do not generate repetitive statements. For example, do not make the same party speak consecutively.\\n\\n- The vessel and the coast guard should speak in turns. Do not make the vessel or the coast guard speak multiple times consecutively. Turns of each party should be separated by \"\\\\n\".\\n\\n- Coast Guard should answer the distress call with one of the following phrases: \"This is Coast Guard\", \"Coast Guard here\", \"Coast Guard responding\".\\n\\n- Distress should not be about fire, explosion, flooding, grounding, sinking, armed attack, person overboard, listing or danger of capsizing.\\n\\nBelow are some example outputs and contexts. Contexts are dictionaries with keys \"vessel_name\", \"vessel_MMSI\", \"vessel_call_sign\", \"vessel_coordinate_dms\", \"digit_by_digit\",\\n\\n\"compass_direction\", \"closest_place_name\", \"distance_to_nearest_place\", \"closest_place_country\", \"distance_to_nearest_port\", \"nearest_port\", \"distance_to_nearest_harbor\",\\n\\n\"nearest_harbor\", \"closest_water_body\" and \"can_have_cargo\".\\n\\n\\n\\nTHIS PART IS VERY IMPORTANT:\\n\\n- You must use \"vessel_name\" and \"vessel_coordinate_dms\".\\n\\n- You must use \"vessel_MMSI\", \"vessel_call_sign\" and \"vessel_type\", as long as they are not null. Omit the null ones.\\n\\n- \"vessel_type\" should be indicated in the format \"[vessel_type] [vessel_name]\". For example, \"motor vessel [vessel_name]\". Do not use phrases \"I am a [vessel_type]\" or \"We are a [vessel_type]\" to indicate the vessel_type.\\n\\n- If \"vessel_MMSI\" is null, the vessel should not indicate its\\' MMSI number. If it is not null, the vessel\\'s MMSI should be indicated in the chatter.\\n\\n- If \"vessel_call_sign\" is null, the vessel should not indicate its\\' call sign or any other call sign in the chatter. If it is not null, the vessel\\'s call sign should be indicated in the chatter.\\n\\n- If \"can_have_cargo\" is null, do not mention any cargo or cargo hold in the chatter. If it is \\'True\\', then you can optionally mention cargo and/or cargo hold. Make the cargo suitable to the vessel type in that case.\\n\\n- If \"digit_by_digit\" is true, all numbers in the chatter should be expressed in digits, for example 454 as \"four five four\". If digit_by_digit is False, use full numbers.\\n\\n- Usage of the rest of the keys are optional.\\n\\n- Do not use both of the information about harbors and ports if they are both present. Use one of them only.\\n\\n- Do not use port and harbor when the vessel gives information about the its position at the beginning of the chatter, instead use them later in chatter optionally.\\n\\n    ### Context:\\n    {\\n    \"vessel_name\": \"CG21126\",\\n    \"vessel_MMSI\": null,\\n    \"vessel_call_sign\": null,\\n    \"vessel_type\": \"Law Enforcement Vessel\",\\n    \"vessel_coordinate_dms\": \"forty degrees three four decimal zero nine minutes North, thirty degrees four two decimal four seven minutes West\",\\n    \"compass_direction\": \"north east\",\\n    \"closest_place_name\": \"Ilha do Corvo\",\\n    \"distance_to_nearest_place\": \"fifty-four\",\\n    \"closest_place_country\": \"Portugal\",\\n    \"distance_to_nearest_port\": \"fifty-seven\",\\n    \"nearest_port\": \"Porto da Casa\",\\n    \"distance_to_nearest_harbor\": \"sixty-nine\",\\n    \"nearest_harbor\": \"Porto de Sao Pedro\",\\n    \"digit_by_digit\": false,\\n    \"can_have_cargo\": null,\\n    \"closest_water_body\": null\\n}\\n\\n    ### Maritime Radio Chatter:\\n    Mayday, Mayday, Mayday. This is law enforcement vessel CG21126. We are at position forty degrees three four decimal zero nine minutes North, thirty degrees four two decimal four seven minutes West. Requesting immediate assistance.\\nThis is Coast Guard. Please elaborate on the nature of your emergency.\\nOur vessel has suffered significant damage from a collision with a large container vessel. We are adrift and need immediate assistance to prevent further casualties.\\nUnderstood. Coast Guard is dispatching assistance from Porto da Casa. ETA is approximately 57 minutes. Maintain current position and remain on this channel for further instructions.<|end_of_text|>'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(raw_dataset[0])\n",
    "all(label == -100 for label in processed_dataset[0]['labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11429\n",
      "1167\n"
     ]
    }
   ],
   "source": [
    "max_len = 0\n",
    "for entry in raw_dataset:\n",
    "    if len(str(entry)) > max_len:\n",
    "        max_len = len(str(entry))\n",
    "print(max_len)         \n",
    "\n",
    "def count_tokens(text, tokenizer):\n",
    "    # Tokenize the text\n",
    "    tokenized_output = tokenizer(text, return_tensors=None)\n",
    "    \n",
    "    # Get the number of tokens\n",
    "    num_tokens = len(tokenized_output[\"input_ids\"])\n",
    "    \n",
    "    return num_tokens\n",
    "str(raw_dataset[0][\"text\"])\n",
    "print(count_tokens(str(raw_dataset[0][\"text\"]), tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset\n",
    "train_dataset = processed_dataset  # Assuming all data is used for training\n",
    "train_dataloader = DataLoader(train_dataset, shuffle=False, collate_fn=default_data_collator, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_to_save = {\n",
    "        \"task_name\": task_name,\n",
    "        \"num_virtual_tokens\": num_virtual_tokens,\n",
    "        \"load_in_4bit\": load_in_4bit,\n",
    "        \"model_name\": used_model,\n",
    "        \"model_path\": f\"{results_path}{quantization}_{current_time}\",\n",
    "        \"chatter_path\": chatter_path,\n",
    "        \"num_epoch\": num_epoch,\n",
    "        \"learning_rate\": lr,\n",
    "        \"prompt\": train_prompt,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3200"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(processed_dataset[0]['labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for name, param in model.named_parameters():\n",
    "#        print(name, param.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU = NVIDIA RTX A6000. Max memory = 47.319 GB.\n",
      "5.846 GB of memory reserved.\n"
     ]
    }
   ],
   "source": [
    "#@title Show current memory stats\n",
    "gpu_stats = torch.cuda.get_device_properties(0)\n",
    "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
    "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
    "print(f\"{start_gpu_memory} GB of memory reserved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:   0%|          | 0/250 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:   0%|          | 0/250 [00:01<?, ?it/s]\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 3.25 GiB. GPU 0 has a total capacity of 47.32 GiB of which 725.94 MiB is free. Including non-PyTorch memory, this process has 46.50 GiB memory in use. Of the allocated memory 45.50 GiB is allocated by PyTorch, and 701.17 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 19\u001b[0m\n\u001b[1;32m     16\u001b[0m loss \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mloss\n\u001b[1;32m     17\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m---> 19\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (step \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m gradient_accumulation_steps \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:  \u001b[38;5;66;03m# Update weights every few steps\u001b[39;00m\n\u001b[1;32m     22\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/miniforge3/envs/chatter_generation/lib/python3.10/site-packages/torch/_tensor.py:626\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    616\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    617\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    618\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    619\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    624\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    625\u001b[0m     )\n\u001b[0;32m--> 626\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    627\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    628\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/chatter_generation/lib/python3.10/site-packages/torch/autograd/__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/chatter_generation/lib/python3.10/site-packages/torch/autograd/graph.py:823\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    821\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    822\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 823\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    824\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    825\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    826\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    827\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 3.25 GiB. GPU 0 has a total capacity of 47.32 GiB of which 725.94 MiB is free. Including non-PyTorch memory, this process has 46.50 GiB memory in use. Of the allocated memory 45.50 GiB is allocated by PyTorch, and 701.17 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "# Optimizer and scheduler\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr)\n",
    "num_training_steps = len(train_dataloader) * num_epoch  \n",
    "lr_scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=num_training_steps)\n",
    "\n",
    "gradient_accumulation_steps = 4\n",
    "\n",
    "# Training loop\n",
    "model.to(device, dtype=torch.bfloat16)\n",
    "model.train()\n",
    "for epoch in range(num_epoch):\n",
    "    total_loss = 0\n",
    "    for step, batch in enumerate(tqdm(train_dataloader, desc=f\"Epoch {epoch + 1}\")):\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        if (step + 1) % gradient_accumulation_steps == 0:  # Update weights every few steps\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            lr_scheduler.step()\n",
    "\n",
    "         # Log metrics to wandb\n",
    "        if is_wandb:\n",
    "            wandb.log({\"epoch\": epoch, \"loss\": loss.item(), \"lr\": optimizer.param_groups[0][\"lr\"]})\n",
    "\n",
    "    avg_loss = total_loss / len(train_dataloader)\n",
    "    print(f\"Epoch {epoch + 1} completed. Average Loss: {avg_loss}\")\n",
    "\n",
    "    # Save model every `save_epochs`\n",
    "    # if (epoch + 1) % save_epochs == 0:\n",
    "    #     model.save_pretrained(os.path.join(save_dir,f'checkpoint{epoch + 1}')) # Local saving\n",
    "    #     tokenizer.save_pretrained(os.path.join(save_dir,f'checkpoint{epoch + 1}'))\n",
    "    #     # Save results to JSON\n",
    "    #     with open(f'{os.path.join(save_dir, f\"checkpoint{epoch + 1}\")}/hyperparameters.json', 'w') as outfile:\n",
    "    #         json.dump(data_to_save, outfile, ensure_ascii=False, indent=4)\n",
    "    #     print(f\"Model saved at epoch {epoch+1} in {os.path.join(save_dir,f'checkpoint{epoch + 1}')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(save_dir) # Local saving\n",
    "tokenizer.save_pretrained(save_dir)\n",
    "# Save results to JSON\n",
    "with open(f'{save_dir}/hyperparameters.json', 'w') as outfile:\n",
    "    json.dump(data_to_save, outfile, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/gakdeniz/Dev/ma-llm-tuning/models/prompt_tuning/reporting_undesignated_distress/unsloth-Llama-3.1-8B_20250323_0728\n"
     ]
    }
   ],
   "source": [
    "print(save_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Peak reserved memory = 37.225 GB.\n",
      "Peak reserved memory for training = 37.225 GB.\n",
      "Peak reserved memory % of max memory = 78.668 %.\n",
      "Peak reserved memory for training % of max memory = 78.668 %.\n"
     ]
    }
   ],
   "source": [
    "#@title Show final memory and time stats\n",
    "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
    "used_percentage = round(used_memory         /max_memory*100, 3)\n",
    "lora_percentage = round(used_memory_for_lora/max_memory*100, 3)\n",
    "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
    "print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
    "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
    "print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TESTING THE PROMPT TUNED MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gakdeniz/Dev/ma-llm-tuning/scripts/geo_test.py:38: DtypeWarning: Columns (8) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  self.geonames_df = pd.read_csv(geonames_data_path, sep='\\t', header=None,\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'prompt_file' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 10\u001b[0m\n\u001b[1;32m      6\u001b[0m ship_data_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/home/gakdeniz/Dev/ma-llm-tuning/data/ship_data_dk_us.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      8\u001b[0m _, scenario_input \u001b[38;5;241m=\u001b[39m geo_test\u001b[38;5;241m.\u001b[39mMaritimeAnalysis(land_shapefile, geonames_data_path, ship_data_path)\u001b[38;5;241m.\u001b[39mexecute([])\n\u001b[0;32m---> 10\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[43mprompt_file\u001b[49m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m     11\u001b[0m     base_prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(f\u001b[38;5;241m.\u001b[39mreadlines())\n\u001b[1;32m     13\u001b[0m prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124mBelow is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\u001b[39m\n\u001b[1;32m     14\u001b[0m \n\u001b[1;32m     15\u001b[0m \u001b[38;5;124m### Instruction:\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;124m### Maritime Radio Chatter:\u001b[39m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\"\"\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'prompt_file' is not defined"
     ]
    }
   ],
   "source": [
    "from peft import PeftModel\n",
    "import geo_test\n",
    "\n",
    "land_shapefile = \"./GSHHS_dataset/GSHHS_shp/f/GSHHS_f_L1.shp\" # path of the GSHHS data\n",
    "geonames_data_path = './all_countries/allCountries.txt' # path of the geonames data\n",
    "ship_data_path = './data/ship_data_dk_us.pkl' # path of the vessel data\n",
    "\n",
    "_, scenario_input = geo_test.MaritimeAnalysis(land_shapefile, geonames_data_path, ship_data_path).execute([])\n",
    "\n",
    "with open(prompt_file, \"r\") as f:\n",
    "    base_prompt = \"\\n\".join(f.readlines())\n",
    "\n",
    "prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{}\n",
    "\n",
    "### Context:\n",
    "{}\n",
    "\n",
    "### Maritime Radio Chatter:\n",
    "{}\"\"\"\n",
    "\n",
    "device = \"cuda\"\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "inputs = tokenizer(\n",
    "[\n",
    "    prompt.format(\n",
    "        chatters[chatters[\"task_name\"]][\"instruction\"] if train_prompt == \"short\" else base_prompt.replace(\"Task: \", \"\"), # instruction\n",
    "        str(scenario_input), # input\n",
    "        \"\", # output - leave this blank for generation!\n",
    "    )\n",
    "], return_tensors = \"pt\").to(\"cuda\")\n",
    "\n",
    "# outputs = model.generate(**inputs, max_new_tokens = 400, use_cache = True)\n",
    "# print(tokenizer.batch_decode(outputs))\n",
    "\n",
    "outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=400,\n",
    "                use_cache = True,\n",
    "                # do_sample=True,\n",
    "                # top_p=0.9,\n",
    "                # temperature=0.9,\n",
    "                # min_length=None,\n",
    "                # top_k=400,\n",
    "                # repetition_penalty=1,\n",
    "                # length_penalty=1,\n",
    "            )\n",
    "output_text = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nYou are a creative expert of maritime industry and undesignated distresses of vessels. Generate a maritime radio chatter which complies with the IMO Standard marine Communication Phrases. A vessel makes a distress call and reports an undesignated distress.\\n\\n\\n\\nTips about creating original scenarios:\\n\\n- Describe different situations of the crew. Be creative.\\n\\n- Mention specific strategies the crew is using to cope with the undesignated distress. Be creative.\\n\\n- Provide ETAs which match with the vessel coordinate and the destination. Provide various types of help available from the Coast Guard. Be creative.\\n\\n- Make the conversation flow naturally. Do not make people say same phrases repeatedly.\\n\\n- Do not generate any other text but the radio chatter. Keep the generation limited to the radio chatter.\\n\\n- All distress calls must start with \"Mayday, Mayday, Mayday\". Ship gives its location in terms of degrees and tells the nature of disaster and the help needed.\\n\\n- Do not generate any other phrases in brackets or paranthesis.\\n\\n- Do not generate repetitive statements. For example, do not make the same party speak consecutively.\\n\\n- The vessel and the coast guard should speak in turns. Do not make the vessel or the coast guard speak multiple times consecutively. Turns of each party should be separated by \"\\\\n\".\\n\\n- Coast Guard should answer the distress call with one of the following phrases: \"This is Coast Guard\", \"Coast Guard here\", \"Coast Guard responding\".\\n\\n- Distress should not be about fire, explosion, flooding, grounding, sinking, armed attack, person overboard, listing or danger of capsizing.\\n\\nBelow are some example outputs and contexts. Contexts are dictionaries with keys \"vessel_name\", \"vessel_MMSI\", \"vessel_call_sign\", \"vessel_coordinate_dms\", \"digit_by_digit\",\\n\\n\"compass_direction\", \"closest_place_name\", \"distance_to_nearest_place\", \"closest_place_country\", \"distance_to_nearest_port\", \"nearest_port\", \"distance_to_nearest_harbor\",\\n\\n\"nearest_harbor\", \"closest_water_body\" and \"can_have_cargo\".\\n\\n\\n\\nTHIS PART IS VERY IMPORTANT:\\n\\n- You must use \"vessel_name\" and \"vessel_coordinate_dms\".\\n\\n- You must use \"vessel_MMSI\", \"vessel_call_sign\" and \"vessel_type\", as long as they are not null. Omit the null ones.\\n\\n- \"vessel_type\" should be indicated in the format \"[vessel_type] [vessel_name]\". For example, \"motor vessel [vessel_name]\". Do not use phrases \"I am a [vessel_type]\" or \"We are a [vessel_type]\" to indicate the vessel_type.\\n\\n- If \"vessel_MMSI\" is null, the vessel should not indicate its\\' MMSI number. If it is not null, the vessel\\'s MMSI should be indicated in the chatter.\\n\\n- If \"vessel_call_sign\" is null, the vessel should not indicate its\\' call sign or any other call sign in the chatter. If it is not null, the vessel\\'s call sign should be indicated in the chatter.\\n\\n- If \"can_have_cargo\" is null, do not mention any cargo or cargo hold in the chatter. If it is \\'True\\', then you can optionally mention cargo and/or cargo hold. Make the cargo suitable to the vessel type in that case.\\n\\n- If \"digit_by_digit\" is true, all numbers in the chatter should be expressed in digits, for example 454 as \"four five four\". If digit_by_digit is False, use full numbers.\\n\\n- Usage of the rest of the keys are optional.\\n\\n- Do not use both of the information about harbors and ports if they are both present. Use one of them only.\\n\\n- Do not use port and harbor when the vessel gives information about the its position at the beginning of the chatter, instead use them later in chatter optionally.\\n\\n### Context:\\n{\\'vessel_name\\': \\'VIVI\\',\\'vessel_MMSI\\': None,\\'vessel_call_sign\\': None,\\'vessel_type\\': \\'Motor Vessel\\',\\'vessel_coordinate_dms\\':\\'seven seven degrees five six minutes North, one zero degrees one six minutes East\\', \\'compass_direction\\':\\'south west\\', \\'closest_place_name\\': \\'Sørøya\\', \\'distance_to_nearest_place\\': \\'four four\\', \\'closest_place_country\\': \\'Svalbard and Jan Mayen\\', \\'distance_to_nearest_port\\': None, \\'nearest_port\\': None, \\'distance_to_nearest_harbor\\': \\'three zero\\', \\'nearest_harbor\\': \\'Levinhamna\\', \\'digit_by_digit\\': True, \\'can_have_cargo\\': None, \\'closest_water_body\\': None}\\n\\n### Maritime Radio Chatter:\\nMayday, Mayday, Mayday. This is motor vessel VIVI, MMSI None, call sign None. I am at seven seven degrees five six minutes North, one zero degrees one six minutes East. I am experiencing a severe engine failure and require immediate assistance. My position is four four nautical miles from Sørøya, Svalbard and Jan Mayen. I have no ETA for assistance. Over.\\nVIVI, this is Coast Guard. We are dispatching a rescue vessel from Levinhamna, ETA three zero hours. Please stand by. Over.\\nThank you, Coast Guard. We will remain stationary until assistance arrives. Over.\\nCopy that, VIVI. Keep this channel open for updates. Over.\\nCoast Guard responding. We have arrived at your position. We are commencing the evacuation process. Over.\\nUnderstood, Coast Guard. We are ready to cooperate. Over.\\nGood luck, VIVI. Keep this channel open for further instructions. Over.\\nCoast Guard out.\\nCopy that, Coast Guard. We are keeping this channel open. Over.\\nStay safe, VIVI. Over.\\nCoast Guard out.\\nCopy that, Coast Guard. Over.\\nGood luck, VIVI. Over.\\nCoast Guard out.\\nCopy that, Coast Guard. Over.\\nStay safe, VIVI. Over.\\nCoast Guard out.\\nCopy that, Coast Guard. We are ready for the evacuation. Over.\\nUnderstood, VIVI. The rescue vessel is approaching your position. Over.\\nThank you, Coast Guard. We will maintain current position. Over.\\nCopy that, VIVI. Keep this channel open for updates. Over.\\nCoast Guard responding. The rescue vessel has arrived at your position. Evacuation is in progress. Over.\\nUnderstood, Coast Guard. We are ready to cooperate. Over.\\nGood luck, VIVI. Keep this channel open for further instructions. Over.\\nCo'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'wandb' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mwandb\u001b[49m\u001b[38;5;241m.\u001b[39mfinish()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'wandb' is not defined"
     ]
    }
   ],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset -f"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chatter_generation",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

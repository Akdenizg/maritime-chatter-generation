import geo_test
import datetime
import json
import os
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel
from transformers import BitsAndBytesConfig
import torch

current_time = datetime.datetime.now().strftime("%Y%m%d_%H%M")

land_shapefile = "./GSHHS_dataset/GSHHS_shp/f/GSHHS_f_L1.shp" # path of the GSHHS data
geonames_data_path = './all_countries/allCountries.txt' # path of the geonames data
ship_data_path = './data/ship_data_dk_us.pkl' # path of the ship data

hyperparameters_path = "./models/.../hyperparameters.json" # path of the hyperparameters.json file generated by lora_finetune notebook.

prompt_file = "./prompts/prompt.txt" # Change to the path of the prompt file for the task category of the data you are finetuning with
save_dir = "./synthetic_chatters"

with open(hyperparameters_path, 'r') as file:
    hyperparameters = json.load(file)

model_name = hyperparameters["model_path"]
training_chatter_path = hyperparameters["chatter_path"]
load_in_4bit = hyperparameters["load_in_4bit"]

quantization_config = BitsAndBytesConfig(
    load_in_4bit=load_in_4bit,  # 4-bit quantization
    bnb_4bit_compute_dtype=torch.bfloat16,  # bfloat16 for Ampere+, float16 for older GPUs
    bnb_4bit_use_double_quant=False,  # Double quantization saves memory
)

with open(prompt_file, "r") as f:
    base_prompt = "\n".join(f.readlines())

# Load the JSON data
with open(training_chatter_path, 'r') as file:
    data = json.load(file)

base_model = AutoModelForCausalLM.from_pretrained("/data/llama-dev/unsloth/Llama-3.1-8B").to("cuda")
model = PeftModel.from_pretrained(model = base_model, model_id=hyperparameters["model_path"], quantization_config = quantization_config if load_in_4bit else None).to("cuda")
tokenizer = AutoTokenizer.from_pretrained(hyperparameters["model_path"])

prompt = """Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.

### Instruction:
{}

### Context:
{}

### Maritime Radio Chatter:
{}"""

results = []
used_mmsi_list = [instance["input"]["vessel_MMSI"] for instance in data[data["task_name"]]["instances"]]

# Run inference 100 times
for i in range(100):
    scenario_inspect, scenario_input = geo_test.MaritimeAnalysis(land_shapefile, geonames_data_path, ship_data_path, data["task_name"]).execute(used_mmsi_list)

    inputs = tokenizer(
        [
            prompt.format(
                data[data['task_name']]['instruction'] if hyperparameters["prompt"] == "short" else base_prompt.replace("Task: ", ""), # instruction
                str(scenario_input), # input
                "", # output - leave this blank for generation!
            )
        ], return_tensors = "pt").to("cuda")

    outputs = model.generate(
                **inputs,
                max_new_tokens=1200,
                use_cache=True,
                # do_sample=True,
                # top_p=0.9,
                # temperature=0.9,
                # min_length=None,
                # top_k=400,
                # repetition_penalty=1,
                # length_penalty=1,
            )
    generated_output = tokenizer.batch_decode(outputs)[0]
    results.append({
        "id": i + 1,
        "inspection": scenario_inspect,
        "input": scenario_input,
        "output": generated_output.replace("<|end_of_text|>","").split("Maritime Radio Chatter:", 1)[1].strip().split("\n\n") if "\n\n" in generated_output.replace("<|end_of_text|>","").split("Maritime Radio Chatter:", 1)[1].strip() else generated_output.replace("<|end_of_text|>","").split("Maritime Radio Chatter:", 1)[1].strip().split("\n")
    })

    i += 1
    print(i)

data_to_save = {
        "task_name": hyperparameters["task_name"],
        "hyperparameters": hyperparameters,
        "results": results,
}
os.makedirs(f'{save_dir}/prompt_tuning/{hyperparameters["task_name"]}', exist_ok=True)
# Save results to JSON
with open(f'{save_dir}/prompt_tuning/{hyperparameters["task_name"]}/{current_time}.json', 'w') as outfile:
    json.dump(data_to_save, outfile, ensure_ascii=False, indent=4)